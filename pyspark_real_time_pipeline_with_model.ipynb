{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95584f35",
   "metadata": {},
   "source": [
    "# pyspark stream with socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d953abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37101619",
   "metadata": {},
   "source": [
    "## Schema 생성\n",
    "\n",
    "입력 string을 dataframe 형식으로 받기 위해 struct schema를 정의."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "815f8eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "userSchema = StructType([\n",
    "    StructField('max_rpm', IntegerType(), True),\n",
    "    StructField('s20', IntegerType(), True),\n",
    "    StructField('s30', IntegerType(), True),\n",
    "    StructField('s40', IntegerType(), True),\n",
    "    StructField('s50', IntegerType(), True),\n",
    "    StructField('s60', IntegerType(), True),\n",
    "    StructField('s70', IntegerType(), True),\n",
    "    StructField('s80', IntegerType(), True),\n",
    "    StructField('s90', IntegerType(), True),\n",
    "    StructField('s100', IntegerType(), True),\n",
    "    StructField('s110', IntegerType(), True),\n",
    "    StructField('s120', IntegerType(), True),\n",
    "    StructField('s130', IntegerType(), True),\n",
    "    StructField('s140', IntegerType(), True),\n",
    "    StructField('s150', IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97422b",
   "metadata": {},
   "source": [
    "## get data by socket\n",
    "\n",
    "port 9999의 localhost socket과 연결(terminal에서 Netcat 이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb331ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/12 13:16:42 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c115d5",
   "metadata": {},
   "source": [
    "## Make DataFrame\n",
    "\n",
    "공백 단위로 구분된 input string을 schema에 따라 dataframe 형태로 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92376210",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.selectExpr(\"split(value, ' ') as data\") \\\n",
    "        .selectExpr(\n",
    "    \"data[0] as max_rpm\",\n",
    "    \"data[1] as s20\",\n",
    "    \"data[2] as s30\",\n",
    "    \"data[3] as s40\",\n",
    "    \"data[4] as s50\",\n",
    "    \"data[5] as s60\",\n",
    "    \"data[6] as s70\",\n",
    "    \"data[7] as s80\",\n",
    "    \"data[8] as s90\",\n",
    "    \"data[9] as s100\",\n",
    "    \"data[9] as s110\",\n",
    "    \"data[9] as s120\",\n",
    "    \"data[9] as s130\",\n",
    "    \"data[9] as s140\",\n",
    "    \"data[9] as s150\"\n",
    ")\n",
    "\n",
    "structuredDF = words.selectExpr(\n",
    "    \"cast(max_rpm as float)\",\n",
    "    \"cast(s20 as integer)\",\n",
    "    \"cast(s30 as integer)\",\n",
    "    \"cast(s40 as integer)\",\n",
    "    \"cast(s50 as integer)\",\n",
    "    \"cast(s60 as integer)\",\n",
    "    \"cast(s70 as integer)\",\n",
    "    \"cast(s80 as integer)\",\n",
    "    \"cast(s90 as integer)\",\n",
    "    \"cast(s100 as integer)\",\n",
    "    \"cast(s110 as integer)\",\n",
    "    \"cast(s120 as integer)\",\n",
    "    \"cast(s130 as integer)\",\n",
    "    \"cast(s140 as integer)\",\n",
    "    \"cast(s150 as integer)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b1975e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structuredDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ff7879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- max_rpm: float (nullable = true)\n",
      " |-- s20: integer (nullable = true)\n",
      " |-- s30: integer (nullable = true)\n",
      " |-- s40: integer (nullable = true)\n",
      " |-- s50: integer (nullable = true)\n",
      " |-- s60: integer (nullable = true)\n",
      " |-- s70: integer (nullable = true)\n",
      " |-- s80: integer (nullable = true)\n",
      " |-- s90: integer (nullable = true)\n",
      " |-- s100: integer (nullable = true)\n",
      " |-- s110: integer (nullable = true)\n",
      " |-- s120: integer (nullable = true)\n",
      " |-- s130: integer (nullable = true)\n",
      " |-- s140: integer (nullable = true)\n",
      " |-- s150: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "structuredDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "911f5786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(structuredDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f00a6",
   "metadata": {},
   "source": [
    "## ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dcd8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9496ff2",
   "metadata": {},
   "source": [
    "저장된 모델(random forest, decision tree)사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad54a068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rfc_loaded_model = CrossValidatorModel.load('random_forest')\n",
    "dtc_loaded_model = CrossValidatorModel.load('decision_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeadab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['max_rpm','s20', 's30', 's40', 's50', 's60', 's70', 's80','s90','s100','s110', 's120','s130','s140','s150']\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=columns, outputCol='features')\n",
    "test_ftr_vec = vec_assembler.transform(structuredDF)\n",
    "prediction = dtc_loaded_model.transform(test_ftr_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae28a8",
   "metadata": {},
   "source": [
    "## Output on Console\n",
    "\n",
    "결과값 console로 출력, probability와 prediction column만 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29af291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = prediction.select(\"probability\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebf3ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/12 13:16:52 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/t6/0yqx06jd2b19jm16pgd8h6p80000gn/T/temporary-0b260d0e-e16b-4853-8a8a-8104ae839309. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/12/12 13:16:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/12/12 13:16:52 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------+----------+\n",
      "|probability|prediction|\n",
      "+-----------+----------+\n",
      "+-----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------+----------+\n",
      "|      probability|prediction|\n",
      "+-----------------+----------+\n",
      "|[0.0,0.0,1.0,0.0]|       2.0|\n",
      "+-----------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------+----------+\n",
      "|      probability|prediction|\n",
      "+-----------------+----------+\n",
      "|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "+-----------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------+----------+\n",
      "|      probability|prediction|\n",
      "+-----------------+----------+\n",
      "|[0.0,1.0,0.0,0.0]|       1.0|\n",
      "+-----------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------------+----------+\n",
      "|      probability|prediction|\n",
      "+-----------------+----------+\n",
      "|[0.0,0.0,0.0,1.0]|       3.0|\n",
      "+-----------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------------+----------+\n",
      "|      probability|prediction|\n",
      "+-----------------+----------+\n",
      "|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "+-----------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/12 13:17:37 WARN TextSocketMicroBatchStream: Stream closed by localhost:9999\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/gyeongbin/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m output_df \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 0: 안전운행, 1: 저위험가속, 2: 중위험가속, 3: 고위험가속\n",
    "query = output_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910704ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
